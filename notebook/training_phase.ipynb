{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio.v2 as imageio\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms as T\n",
    "from torchvision.transforms import functional as TF\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "#from torchsummary import summary\n",
    "from tqdm import tqdm\n",
    "#from cityscapesscripts.helpers.labels import trainId2label as t2l\n",
    "import cv2\n",
    "#from sklearn.model_selection import train_test_split\n",
    "from torchmetrics.classification import MulticlassJaccardIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.segmentation import deeplabv3_resnet50, DeepLabV3_ResNet50_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_20 = { \n",
    "        0: 0,\n",
    "        1: 0,\n",
    "        2: 0,\n",
    "        3: 0,\n",
    "        4: 0,\n",
    "        5: 0,\n",
    "        6: 0,\n",
    "        7: 1,\n",
    "        8: 2,\n",
    "        9: 0,\n",
    "        10: 0,\n",
    "        11: 3,\n",
    "        12: 4,\n",
    "        13: 5,\n",
    "        14: 0,\n",
    "        15: 0,\n",
    "        16: 0,\n",
    "        17: 6,\n",
    "        18: 0,\n",
    "        19: 7,\n",
    "        20: 8,\n",
    "        21: 9,\n",
    "        22: 10,\n",
    "        23: 11,\n",
    "        24: 12,\n",
    "        25: 13,\n",
    "        26: 14,\n",
    "        27: 15,\n",
    "        28: 16,\n",
    "        29: 0,\n",
    "        30: 0,\n",
    "        31: 17,\n",
    "        32: 18,\n",
    "        33: 19,\n",
    "        -1: 0\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class RandomCropper:\n",
    "\n",
    "    def __init__(self, crop_size = (256,256)):\n",
    "        self.crop_size = crop_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image = sample['images']\n",
    "        mask = sample['masks']\n",
    "\n",
    "        h, w, _ = image.shape\n",
    "        top = random.randint(0, h - self.crop_size[0])\n",
    "        left = random.randint(0, w - self.crop_size[1])\n",
    "\n",
    "        sample['images'] = image[top:top+self.crop_size[0], left:left+self.crop_size[1], :]\n",
    "        sample['masks'] = mask[top:top+self.crop_size[0], left:left+self.crop_size[1]]\n",
    "\n",
    "        return sample\n",
    "    \n",
    "class HorizontalFlip:\n",
    "\n",
    "    def __init__(self, keys):\n",
    "        self.keys = keys\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        for key in self.keys:\n",
    "#                 sample[key] = sample[key].transpose(sample[key].FLIP_LEFT_RIGHT)\n",
    "                sample[key] = cv2.flip(sample[key], 1)\n",
    "        return sample\n",
    "\n",
    "class VerticalFlip:\n",
    "\n",
    "    def __init__(self, keys):\n",
    "        self.keys = keys\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        for key in self.keys:\n",
    "#                 sample[key] = sample[key].transpose(sample[key].FLIP_LEFT_RIGHT)\n",
    "                sample[key] = cv2.flip(sample[key], 0)\n",
    "        return sample\n",
    "\n",
    "class RandomRotator:\n",
    "    def __init__(self, keys):\n",
    "        self.keys = keys\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        for key in self.keys:\n",
    "            image = sample[key]  # Get the image from the key\n",
    "            angle = np.random.randint(-30, 30)  # Random angle between -30 and 30 degrees\n",
    "\n",
    "            # Perform rotation\n",
    "            h, w = image.shape[:2]\n",
    "            center = (w // 2, h // 2)\n",
    "            rotation_matrix = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "            rotated_image = cv2.warpAffine(image, rotation_matrix, (w, h))\n",
    "\n",
    "            sample[key] = rotated_image  # Update the sample with the rotated image\n",
    "\n",
    "        return sample\n",
    "\n",
    "class ImageNormalizer():\n",
    "    def __init__(self, keys):\n",
    "        self.keys = keys\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        for key in self.keys:\n",
    "            images = sample[key].astype(np.float32)\n",
    "            images /= 255.0\n",
    "            images -= images.mean(axis=(0, 1), keepdims=True)\n",
    "            images /= images.std(axis=(0, 1), keepdims=True)\n",
    "            sample[key] = images\n",
    "\n",
    "        return sample\n",
    "    \n",
    "import torch\n",
    "\n",
    "class ToTensor():\n",
    "    \n",
    "    def __init__(self,keys):\n",
    "        self.keys=keys\n",
    "        \n",
    "    def __call__(self,sample):\n",
    "        for key in self.keys:\n",
    "            sample[key]= torch.tensor(sample[key],dtype=torch.long)\n",
    "            if key == 'image_paths':\n",
    "                sample[key] = sample[key].permute(2, 0, 1)\n",
    "                \n",
    "        return sample\n",
    "class OneHotEncodeLabels:\n",
    "    \n",
    "    def __init__(self, keys):\n",
    "        self.keys = keys\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        for key in self.keys:\n",
    "            tens = sample[key]\n",
    "            tens = torch.where(tens == 255, 20, tens)\n",
    "            # print(tens.shape)\n",
    "            # tens_max = torch.argmax(tens, dim=1)\n",
    "            # print(tens_max)\n",
    "            sample[key] = torch.nn.functional.one_hot(tens.long(), num_classes=20).float()\n",
    "            sample[key] = sample[key].permute(2, 0, 1)\n",
    "            \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CityscapesDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_dir,transforms= None):\n",
    "        #self.labelcolorpaths=[]\n",
    "        self.image_dir=image_dir\n",
    "        self.label_dir=label_dir\n",
    "        self.transform=transforms\n",
    "        self.imagepaths=sorted(glob.glob(self.image_dir))\n",
    "        labelpaths=sorted(glob.glob(self.label_dir))\n",
    "        self.label_paths=[]\n",
    "        for img in labelpaths:\n",
    "            if 'labelIds' in os.path.basename(img):\n",
    "                self.label_paths.append(img)\n",
    "     \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.imagepaths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #print(self.imagepaths[idx])           \n",
    "        image = Image.open(self.imagepaths[idx]).convert(\"RGB\")\n",
    "        mask = imageio.imread(self.label_paths[idx])    \n",
    "        \n",
    "        \n",
    "        image = image.resize((512, 256))\n",
    "        mask = cv2.resize(mask, (512,256))\n",
    "        \n",
    "        for i, j in np.ndindex(mask.shape):\n",
    "            mask[i][j] = mapping_20[mask[i][j]]\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            #image = Image.fromarray(image)\n",
    "            image = self.transform(image)\n",
    "            #image = ToTensor()(image)\n",
    "        \n",
    "        mask = torch.tensor(mask, dtype=torch.uint8)\n",
    "        mask = torch.tensor(mask.tolist())\n",
    "        \n",
    "#         mask_color = torch.tensor(mask_color, dtype=torch.float32)\n",
    "#         mask_color = torch.tensor(mask_color.tolist())\n",
    "#         mask_color = mask_color.permute(2, 0, 1)\n",
    "        \n",
    "        return image, mask#, mask_color "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "transform=DeepLabV3_ResNet50_Weights.COCO_WITH_VOC_LABELS_V1.transforms()\n",
    "\n",
    "batch_size = 1\n",
    "NUM_CLASSES = 20\n",
    "start_epoch = 0\n",
    "epochs = 1\n",
    "learning_rate = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SemanticSegmentation(\n",
       "    resize_size=[520]\n",
       "    mean=[0.485, 0.456, 0.406]\n",
       "    std=[0.229, 0.224, 0.225]\n",
       "    interpolation=InterpolationMode.BILINEAR\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "train_dir = '/app/duy55/segmentation/cityscapes/Cityspaces/images/train/*/*'\n",
    "label_train_dir = '/app/duy55/segmentation/cityscapes/Cityspaces/gtFine/train/*/*'\n",
    "\n",
    "val_dir = '/app/duy55/segmentation/cityscapes/Cityspaces/images/val/*/*'\n",
    "label_val_dir = '/app/duy55/segmentation/cityscapes/Cityspaces/gtFine/val/*/*'\n",
    "\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "\n",
    "\n",
    "train_dataset = CityscapesDataset(image_dir=train_dir, label_dir=label_train_dir,transforms=transform)\n",
    "valid_dataset = CityscapesDataset(image_dir=val_dir, label_dir=label_val_dir,transforms=transform)\n",
    "\n",
    "valid_dataset, test_valid_dataset = torch.utils.data.random_split(valid_dataset, [0.8, 0.2])\n",
    "\n",
    "test_dataset = test_valid_dataset\n",
    "\n",
    "#divisione del dataset da aggiustare\n",
    "#train_dataset.__getitem__(3)    \n",
    "# Get train and val data loaders\n",
    "test_loader = DataLoader(test_dataset)\n",
    "train_loader = DataLoader(train_dataset,batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(valid_dataset,batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print (len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.segmentation.deeplabv3 import DeepLabHead\n",
    "from torchvision import models\n",
    "\n",
    "\n",
    "def createDeepLabv3(outputchannels=20):\n",
    "    \"\"\"DeepLabv3 class with custom head\n",
    "\n",
    "    Args:\n",
    "        outputchannels (int, optional): The number of output channels\n",
    "        in your dataset masks. Defaults to 1.\n",
    "\n",
    "    Returns:\n",
    "        model: Returns the DeepLabv3 model with the ResNet101 backbone.\n",
    "    \"\"\"\n",
    "    model = models.segmentation.deeplabv3_resnet50(pretrained=True,progress=True)\n",
    "    model.classifier = DeepLabHead(2048, outputchannels)\n",
    "    # Set the model in training mode\n",
    "    #model.train()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FocalLoss(torch.nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        ce_loss = F.cross_entropy(input, target, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
    "        \n",
    "        return focal_loss.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "#si deve aggiustare IoU\n",
    "def train(epochs, start_epoch, model, train_loader, val_loader, loss, optimizer, patch=False):\n",
    "    jaccard_mean = MulticlassJaccardIndex(num_classes=NUM_CLASSES, ignore_index=0).to(device)\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    val_loss = []\n",
    "    val_iou = []\n",
    "    train_iou = []\n",
    "    min_loss = np.inf\n",
    "    \n",
    "    fit_time = time.time()\n",
    "    \n",
    "    for e in range(start_epoch, epochs):\n",
    "        since = time.time()\n",
    "        running_loss = 0\n",
    "        iou_score = 0\n",
    "        \n",
    "        #training loop\n",
    "        model.train()\n",
    "        for data, target in tqdm(train_loader):\n",
    "            #training phase\n",
    "\n",
    "            mask= target.squeeze()\n",
    "            data = data.to(device)\n",
    "            mask = mask.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            #forward\n",
    "            \n",
    "            output = model(torch.squeeze(data, dim=1))[\"out\"].to(device)\n",
    "            # Loss and IoU evaluation\n",
    "            loss = loss_function(output, mask)\n",
    "            iou_score += jaccard_mean(output, mask).item()\n",
    "            # backward\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_iou_score = 0\n",
    "        #validation loop\n",
    "        with torch.no_grad():\n",
    "            for data, target in tqdm(val_loader):\n",
    "                output = model(data.to(device))\n",
    "                mask= target.squeeze()\n",
    "                mask = mask.to(device)\n",
    "                \n",
    "                #Loss and IoU evaluation\n",
    "                val_iou_score +=  jaccard_mean(output, mask).item()\n",
    "                loss = loss_function(output, mask)                          \n",
    "                val_loss += loss.item()\n",
    "            \n",
    "        train_losses.append(running_loss/len(train_loader))\n",
    "        test_losses.append(val_loss/len(val_loader))\n",
    "\n",
    "        \n",
    "        if min_loss > (val_loss/len(val_loader)):\n",
    "            print('Loss Decreasing.. {:.3f} >> {:.3f} '.format(min_loss, (val_loss/len(val_loader))))\n",
    "            min_loss = (val_loss/len(val_loader))\n",
    "\n",
    "            print('saving model...')\n",
    "            torch.save(model.state_dict(), 'UNet_best.pt')\n",
    "\n",
    "\n",
    "        # Viene salvata la IoU ad ogni epoca sia per il training che per il validation\n",
    "        val_iou.append(val_iou_score/len(val_loader))\n",
    "        train_iou.append(iou_score/len(train_loader))\n",
    "\n",
    "        print(\"Epoch:{}/{}..\".format(e+1, epochs),\n",
    "              \"Train Loss: {:.3f}..\".format(running_loss/len(train_loader)),\n",
    "              \"Val Loss: {:.3f}..\".format(val_loss/len(val_loader)),\n",
    "              \"Train mIoU:{:.3f}..\".format(iou_score/len(train_loader)),\n",
    "              \"Val mIoU: {:.3f}..\".format(val_iou_score/len(val_loader)),\n",
    "              \"Time: {:.2f}m\".format((time.time()-since)/60))\n",
    "        print('saving model...')\n",
    "        checkpoint(model, f\"epoch-{e}.pth\")\n",
    "        \n",
    "    history = {'train_loss' : train_losses, 'val_loss': test_losses,\n",
    "               'train_miou' :train_iou, 'val_miou':val_iou,\n",
    "              }\n",
    "    print('Total time: {:.2f} m' .format((time.time()- fit_time)/60))\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DeepLabV3_ResNet50_Weights.COCO_WITH_VOC_LABELS_V1`. You can also use `weights=DeepLabV3_ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/deeplabv3_resnet50_coco-cd0a2569.pth\" to /root/.cache/torch/hub/checkpoints/deeplabv3_resnet50_coco-cd0a2569.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c013a46821e04ed5b38721f0c158d0ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/161M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model=createDeepLabv3(outputchannels=20)\n",
    "model=model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model=createDeepLabv3(outputchannels=20)\n",
    "model=model.to(device)\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss(ignore_index=0)\n",
    "loss_function = loss_function.to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2975 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected more than 1 value per channel when training, got input size torch.Size([1, 256, 1, 1])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m  \n",
      "Cell \u001b[0;32mIn[10], line 31\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(epochs, start_epoch, model, train_loader, val_loader, loss, optimizer, patch)\u001b[0m\n\u001b[1;32m     28\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m#forward\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mout\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Loss and IoU evaluation\u001b[39;00m\n\u001b[1;32m     33\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_function(output, mask)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchvision/models/segmentation/_utils.py:27\u001b[0m, in \u001b[0;36m_SimpleSegmentationModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     25\u001b[0m result \u001b[38;5;241m=\u001b[39m OrderedDict()\n\u001b[1;32m     26\u001b[0m x \u001b[38;5;241m=\u001b[39m features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mout\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 27\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39minterpolate(x, size\u001b[38;5;241m=\u001b[39minput_shape, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbilinear\u001b[39m\u001b[38;5;124m\"\u001b[39m, align_corners\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     29\u001b[0m result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mout\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m x\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchvision/models/segmentation/deeplabv3.py:111\u001b[0m, in \u001b[0;36mASPP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    109\u001b[0m _res \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m conv \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvs:\n\u001b[0;32m--> 111\u001b[0m     _res\u001b[38;5;241m.\u001b[39mappend(\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    112\u001b[0m res \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(_res, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproject(res)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torchvision/models/segmentation/deeplabv3.py:81\u001b[0m, in \u001b[0;36mASPPPooling.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     79\u001b[0m size \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:]\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m---> 81\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mmod\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39minterpolate(x, size\u001b[38;5;241m=\u001b[39msize, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbilinear\u001b[39m\u001b[38;5;124m\"\u001b[39m, align_corners\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/batchnorm.py:171\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    164\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:2448\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2435\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   2436\u001b[0m         batch_norm,\n\u001b[1;32m   2437\u001b[0m         (\u001b[38;5;28minput\u001b[39m, running_mean, running_var, weight, bias),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2445\u001b[0m         eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[1;32m   2446\u001b[0m     )\n\u001b[1;32m   2447\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[0;32m-> 2448\u001b[0m     \u001b[43m_verify_batch_size\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2450\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mbatch_norm(\n\u001b[1;32m   2451\u001b[0m     \u001b[38;5;28minput\u001b[39m, weight, bias, running_mean, running_var, training, momentum, eps, torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mcudnn\u001b[38;5;241m.\u001b[39menabled\n\u001b[1;32m   2452\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:2416\u001b[0m, in \u001b[0;36m_verify_batch_size\u001b[0;34m(size)\u001b[0m\n\u001b[1;32m   2414\u001b[0m     size_prods \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m size[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m   2415\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_prods \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m-> 2416\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected more than 1 value per channel when training, got input size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(size))\n",
      "\u001b[0;31mValueError\u001b[0m: Expected more than 1 value per channel when training, got input size torch.Size([1, 256, 1, 1])"
     ]
    }
   ],
   "source": [
    "history = train(epochs, start_epoch, model, train_loader, val_loader, loss_function, optimizer)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
